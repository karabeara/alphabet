{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/NeilNie/EMNIST-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karabressler/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import os\n",
    "import argparse\n",
    "import keras\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "# from skimage.io import imread, imsave\n",
    "from PIL import Image\n",
    "from keras.models import save_model, Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, Convolution2D, Dropout, Dense, Flatten, LSTM\n",
    "\n",
    "# Mute tensorflow debugging information console\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(mat_file_path, width=28, height=28, max=None):\n",
    "\n",
    "    ''' Load data in from .mat file as specified by the paper.\n",
    "        Arguments:\n",
    "            mat_file_path: path to the .mat, should be in sample/\n",
    "        Optional Arguments:\n",
    "            width: specified width\n",
    "            height: specified height\n",
    "            max: the max number of samples to load\n",
    "            verbose: enable verbose printing\n",
    "        Returns:\n",
    "            A tuple of training and test data, and the mapping for class code to ascii value,\n",
    "            in the following format:\n",
    "                - ((training_images, training_labels), (testing_images, testing_labels), mapping)\n",
    "    '''\n",
    "    # Local functions\n",
    "    def rotate(img):\n",
    "        # Used to rotate images (for some reason they are transposed on read-in)\n",
    "        flipped = np.fliplr(img)\n",
    "        return np.rot90(flipped)\n",
    "\n",
    "    def display(img, threshold=0.5):\n",
    "        # Debugging only\n",
    "        render = ''\n",
    "        for row in img:\n",
    "            for col in row:\n",
    "                if col > threshold:\n",
    "                    render += '@'\n",
    "                else:\n",
    "                    render += '.'\n",
    "            render += '\\n'\n",
    "        return render\n",
    "\n",
    "    # Load convoluted list structure form loadmat\n",
    "    mat = loadmat(mat_file_path)\n",
    "\n",
    "    # Load char mapping\n",
    "    mapping = {kv[0]:kv[1:][0] for kv in mat['dataset'][0][0][2]}\n",
    "    pickle.dump(mapping, open('bin/mapping.p', 'wb' ))\n",
    "\n",
    "    # Load training data\n",
    "    if max == None:\n",
    "        max = len(mat['dataset'][0][0][0][0][0][0])\n",
    "    training_images = mat['dataset'][0][0][0][0][0][0][:max].reshape(max, height, width, 1)\n",
    "    training_labels = mat['dataset'][0][0][0][0][0][1][:max]\n",
    "\n",
    "    # Load testing data\n",
    "    if max == None:\n",
    "        max = len(mat['dataset'][0][0][1][0][0][0])\n",
    "    else:\n",
    "        max = int(max / 6)\n",
    "    testing_images = mat['dataset'][0][0][1][0][0][0][:max].reshape(max, height, width, 1)\n",
    "    testing_labels = mat['dataset'][0][0][1][0][0][1][:max]\n",
    "\n",
    "    # Reshape training data to be valid\n",
    "    _len = len(training_images)\n",
    "    for i in range(len(training_images)):\n",
    "        training_images[i] = rotate(training_images[i])\n",
    "\n",
    "    # Reshape testing data to be valid\n",
    "    _len = len(testing_images)\n",
    "    for i in range(len(testing_images)):\n",
    "        testing_images[i] = rotate(testing_images[i])\n",
    "\n",
    "    # Convert type to float32\n",
    "    training_images = training_images.astype('float32')\n",
    "    testing_images = testing_images.astype('float32')\n",
    "\n",
    "    # Normalize to prevent issues with model\n",
    "    training_images /= 255\n",
    "    testing_images /= 255\n",
    "\n",
    "    nb_classes = len(mapping)\n",
    "\n",
    "    return ((training_images, training_labels), (testing_images, testing_labels), mapping, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(training_data, width=28, height=28):\n",
    "\n",
    "    ''' Build and train neural network. Also offloads the net in .yaml and the\n",
    "        weights in .h5 to the bin/.\n",
    "        Arguments:\n",
    "            training_data: the packed tuple from load_data()\n",
    "        Optional Arguments:\n",
    "            width: specified width\n",
    "            height: specified height\n",
    "            epochs: the number of epochs to train over\n",
    "            verbose: enable verbose printing\n",
    "    '''\n",
    "    # Initialize data\n",
    "    (x_train, y_train), (x_test, y_test), mapping, nb_classes = training_data\n",
    "    input_shape = (height, width, 1)\n",
    "\n",
    "    # Hyperparameters\n",
    "    nb_filters = 32 # number of convolutional filters to use\n",
    "    pool_size = (2, 2) # size of pooling area for max pooling\n",
    "    kernel_size = (3, 3) # convolution kernel size\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, kernel_size, padding='valid', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Convolution2D(nb_filters, kernel_size, activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    # print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_data, callback=True, batch_size=256, epochs=10):\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test), mapping, nb_classes = training_data\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = np.subtract(y_train, 1)\n",
    "    y_test  = np.subtract(y_test, 1)\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test  = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    if callback == True:\n",
    "        # Callback for analysis in TensorBoard\n",
    "        tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[tbCallBack] if callback else None)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Offload model to file\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(\"bin/model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    save_model(model, 'bin/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124800 samples, validate on 20800 samples\n",
      "Epoch 1/10\n",
      "124800/124800 [==============================] - 349s 3ms/step - loss: 0.7465 - acc: 0.7727 - val_loss: 0.3046 - val_acc: 0.9042\n",
      "Epoch 2/10\n",
      "124800/124800 [==============================] - 328s 3ms/step - loss: 0.3414 - acc: 0.8904 - val_loss: 0.2436 - val_acc: 0.9192\n",
      "Epoch 3/10\n",
      "124800/124800 [==============================] - 337s 3ms/step - loss: 0.2789 - acc: 0.9090 - val_loss: 0.2246 - val_acc: 0.9273\n",
      "Epoch 4/10\n",
      "124800/124800 [==============================] - 370s 3ms/step - loss: 0.2461 - acc: 0.9189 - val_loss: 0.2061 - val_acc: 0.9339\n",
      "Epoch 5/10\n",
      "124800/124800 [==============================] - 324s 3ms/step - loss: 0.2265 - acc: 0.9235 - val_loss: 0.2057 - val_acc: 0.9319\n",
      "Epoch 6/10\n",
      "124800/124800 [==============================] - 312s 3ms/step - loss: 0.2090 - acc: 0.9285 - val_loss: 0.2019 - val_acc: 0.9352\n",
      "Epoch 7/10\n",
      "124800/124800 [==============================] - 311s 2ms/step - loss: 0.1978 - acc: 0.9331 - val_loss: 0.1969 - val_acc: 0.9360\n",
      "Epoch 8/10\n",
      "124800/124800 [==============================] - 312s 3ms/step - loss: 0.1879 - acc: 0.9355 - val_loss: 0.1891 - val_acc: 0.9375\n",
      "Epoch 9/10\n",
      "124800/124800 [==============================] - 311s 2ms/step - loss: 0.1784 - acc: 0.9387 - val_loss: 0.1895 - val_acc: 0.9387\n",
      "Epoch 10/10\n",
      "124800/124800 [==============================] - 315s 3ms/step - loss: 0.1702 - acc: 0.9412 - val_loss: 0.1860 - val_acc: 0.9390\n",
      "Test score: 0.1860164220572915\n",
      "Test accuracy: 0.9390384615384615\n"
     ]
    }
   ],
   "source": [
    "mat_file_path = \"dataset/matlab/emnist-letters.mat\"\n",
    "training_data = load_data(mat_file_path)\n",
    "model = build_model(training_data)\n",
    "train(model, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(training_images, training_labels), (testing_images, testing_labels), mapping, nb_classes = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, best_confidences, best_responses, x_start, y_start):\n",
    "    x_ = x\n",
    "    imsave('resized.png', x)\n",
    "    x = imresize(x,(28,28))\n",
    "\n",
    "    # reshape image data for use in neural network\n",
    "    x = x.reshape(1,28,28,1)\n",
    "\n",
    "    # Convert type to float32\n",
    "    x = x.astype('float32')\n",
    "\n",
    "    # Normalize to prevent issues with model\n",
    "    x /= 255\n",
    "\n",
    "    # Predict from model\n",
    "    out = model.predict(x)\n",
    "\n",
    "    # Generate response\n",
    "    argmax      = int(np.argmax(out, axis=1)[0])\n",
    "    prediction  = chr(mapping[argmax + 1])\n",
    "    confidence  = np.float64(str(max(out[0]) * 100)[:6])\n",
    "\n",
    "    response = {'prediction': prediction,\n",
    "                'confidence': confidence,\n",
    "                'box': (x_start, y_start),\n",
    "                'sample_len': x_.shape, \n",
    "                'image': x_}\n",
    "\n",
    "    if confidence > best_confidences[argmax]:\n",
    "        best_confidences[argmax] = confidence\n",
    "        best_responses[argmax]   = response\n",
    "        \n",
    "    return best_confidences, best_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karabressler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/Users/karabressler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/karabressler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "112\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "im_full  = \"locations/italy_3.jpg\"\n",
    "im_saved = \"saved.jpg\"\n",
    "img = Image.open(im_full) \n",
    "\n",
    "num_classes = 26\n",
    "best_confidences = np.zeros(num_classes)\n",
    "best_responses   = [dict() for x in range(num_classes)]\n",
    "\n",
    "index = 0\n",
    "step  = 4\n",
    "ratio = 1\n",
    "sample_len = 28\n",
    "\n",
    "width  = img.size[0]\n",
    "height = img.size[1]\n",
    "\n",
    "# 28 x 28\n",
    "while sample_len < np.minimum(width,height):\n",
    "    print(sample_len)\n",
    "    \n",
    "    start_i = width - sample_len\n",
    "    start_j = height - sample_len\n",
    "    \n",
    "    i_passes = int(np.floor(start_i/step))\n",
    "    j_passes = int(np.floor(start_j/step))\n",
    "    \n",
    "    for i in range( i_passes ): \n",
    "        for j in range( j_passes ):\n",
    "\n",
    "            step_i = step*i;\n",
    "            step_j = step*j;\n",
    "\n",
    "            im_cropped = img.crop( (step_i, step_j, step_i+sample_len, step_j+sample_len) )\n",
    "            im_cropped.save(im_saved)\n",
    "            x = imread(im_saved, mode='L')\n",
    "\n",
    "            best_confidences, best_responses = predict(x, best_confidences, best_responses, step_i, step_j)\n",
    "\n",
    "            x = np.invert(x)\n",
    "            best_confidences, best_responses = predict(x, best_confidences, best_responses, step_i, step_j)\n",
    "    \n",
    "    index += 1\n",
    "    ratio = np.power(2, index)\n",
    "    sample_len = 28 * ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (num_classes): \n",
    "    if not best_responses[i]:\n",
    "        print(\"EMPTY\")\n",
    "    else:\n",
    "        x_ = best_responses[i]['image']\n",
    "#         plt.imshow(x_)\n",
    "#         plt.show()\n",
    "\n",
    "        response = {'prediction': best_responses[i]['prediction'],\n",
    "                    'confidence': best_responses[i]['confidence'],\n",
    "                    'sample_len': best_responses[i]['sample_len'],\n",
    "                    'box': best_responses[i]['box']}\n",
    "        print(response)\n",
    "        \n",
    "        im = Image.fromarray(best_responses[i]['image'])\n",
    "        im.save(\"attempts/attempt_9/\" + best_responses[i]['prediction'] + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# import tensorflow as tf\n",
    "# from PIL import Image\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "# sess = tf.Session()\n",
    "# sess.run(init)\n",
    "\n",
    "# im_full  = \"naples.jpg\"\n",
    "# im_saved = \"saved.jpg\"\n",
    "# img = Image.open(im_full) \n",
    "\n",
    "# img.show()\n",
    "\n",
    "# step = 4\n",
    "\n",
    "# width = img.size[0]\n",
    "# height = img.size[1]\n",
    "\n",
    "# width_passes  = int(np.floor(width/step))\n",
    "# height_passes = int(np.floor(height/step))\n",
    "\n",
    "# for i in range( width_passes ): \n",
    "#     for j in range( height_passes ):\n",
    "        \n",
    "#         step_i = step*i;\n",
    "#         step_j = step*j;\n",
    "        \n",
    "#         im_cropped = img.crop( (step_i, step_j, step_i+28, step_j+28) )\n",
    "#         im_cropped.save(im_saved)\n",
    "\n",
    "#         print(type(im_cropped))\n",
    "        \n",
    "#         region = scipy.ndimage.imread(im_saved, flatten=True)\n",
    "#         data = np.vectorize(lambda x: 255 - x)(np.ndarray.flatten(region))\n",
    "#         result = sess.run(tf.argmax(y,1), feed_dict={x: [data]})\n",
    "        \n",
    "#         prediction = ' '.join(map(str, result))\n",
    "        #region = region.reshape([28,28])\n",
    "        #plt.gray()\n",
    "#         if (prediction == 3):\n",
    "#             plt.imshow(region)\n",
    "#             plt.show()\n",
    "#             print (prediction)\n",
    "\n",
    "# print ({x: [data]})\n",
    "\n",
    "# image = Image.open(im_saved) \n",
    "# image.show()\n",
    "\n",
    "\n",
    "\n",
    "# im_full  = \"naples.jpg\"\n",
    "# im_saved = \"saved.jpg\"\n",
    "# img = Image.open(im_full) \n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# print (prediction)\n",
    "\n",
    "# Test (28,28) image\n",
    "# im_cropped = img.crop( (400,600,428,628) )\n",
    "# im_cropped.save(im_saved)\n",
    "\n",
    "# x = imread('naples.jpg', mode='L')\n",
    "# x = imread('saved.jpg', mode='L')\n",
    "# x = np.invert(x)\n",
    "\n",
    "# x = imread('nums_bw/e_1.png', mode='L')\n",
    "# x = imread('letters_small/l.png', mode='L')\n",
    "# x = imread('letters_small/b.png', mode='L')\n",
    "# x = np.invert(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensorflow",
   "language": "python",
   "name": "env_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
